{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Ztc93UsbuKh"
   },
   "source": [
    "# DIGI405: Texts, Discourses and Data\n",
    "## Exploring Word and Document Representations in The Anglo-Saxon Chronicle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how we can use token counts and Tf-idf scores as ways to represent words and documents. With these scores, we an begin to measure word and document similarity. We will use cosine similarity for this similarity measure, and will apply k-means clustering. These methods are not a topic we focus on in DIGI405, but they are a good way to illustrate the different ways we can represent documents. \n",
    "\n",
    "The data we will be using is an historical document of English history, know as the Anglo-Saxon Chronicle. It was compiled in the 9th century, but was probably written down over a longer period by many hands. It contains a lot of words and phrases that repeat, which helps us to see the similarity when documents are clustered into groups together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6r5dgEnh4pHJ"
   },
   "source": [
    "### Load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1321,
     "status": "ok",
     "timestamp": 1707601273923,
     "user": {
      "displayName": "Christopher Thomson",
      "userId": "06069808385232535167"
     },
     "user_tz": -780
    },
    "id": "gesn-K-zt2pk",
    "outputId": "150ccd8a-ce26-4d08-8d51-bb33b9e4cf2b"
   },
   "outputs": [],
   "source": [
    "# In case you need to install libraries\n",
    "#!pip install nltk\n",
    "#!pip install textblob\n",
    "#!pip install -U spacy\n",
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 314,
     "status": "ok",
     "timestamp": 1707601358410,
     "user": {
      "displayName": "Christopher Thomson",
      "userId": "06069808385232535167"
     },
     "user_tz": -780
    },
    "id": "gunCYIlx4kvu"
   },
   "outputs": [],
   "source": [
    "# open our text file, and store it in a variable\n",
    "# the path to the file must point to its location in your Google Drive.\n",
    "with open('Anglo-Saxon_Chronicle.txt') as f:\n",
    "  text = f.read()\n",
    "  # split the text into a list of words (simply using spaces to tokenise words)\n",
    "  words = text.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 448,
     "status": "ok",
     "timestamp": 1707601362976,
     "user": {
      "displayName": "Christopher Thomson",
      "userId": "06069808385232535167"
     },
     "user_tz": -780
    },
    "id": "y7NbYsVa5B9d",
    "outputId": "27f3ab02-cfc8-4431-e25b-80ba0d9ecf7e"
   },
   "outputs": [],
   "source": [
    "# Look at the first 50 items in our list of words\n",
    "print(words[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xRcP8cu1d-2R"
   },
   "source": [
    "{1} Let's count how many of each word we have in the Chronicle overall. In the code below, `Counter`provides a ready-made method to create and sort a frequency list. We will call it 'prelim_count' as this is just our first look at counting the words. We can specify how much of the frequency list to display (50 at present)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r7CgIPSKZ8Ii"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "prelim_count = collections.Counter(words)\n",
    "prelim_count.most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGNNJLhGdnMT"
   },
   "source": [
    "This word frequency list seems like a useful start. For example, we can see that 'King' was much more frequently mentioned than 'Bishop', or 'Earl', and that deaths are frequently recorded.\n",
    "\n",
    "But perhaps you can already see some problems with this list. What do you notice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gM7rQAhrhjva"
   },
   "source": [
    "### Pre-processing\n",
    "\n",
    "First, let's split our text into separate annals, or records, typically one per year. Then we will lowercase and tokenise each record. We will also remove many words based on their Penn Treebank part of speech tags. \n",
    "\n",
    "| processing step | effect |\n",
    "| --- | --- |\n",
    "| tokenising | spaCy tokeniser (iterating over spaCy Doc object). This is much better than ```text.split(' ')``` used above. |\n",
    "| filter POS | Remove proper names, grammatical words and punctuation. |\n",
    "| lowercasing | Normalise text to treat 'Year' and 'year' as the same token. |\n",
    "| exclude non-alphanumeric | Remove any symbols, eg '%' if any |\n",
    "| exclude specific stopwords | Exclude any other words we want to remove |\n",
    "\n",
    "\n",
    "For reference, the Penn Treebank part of speech tags can be found here:\n",
    "https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html \n",
    "\n",
    "We'll create two versions of our corpus, one where each document is a list of tokens, and one where these are joined together and more readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 30427,
     "status": "ok",
     "timestamp": 1707602305118,
     "user": {
      "displayName": "Christopher Thomson",
      "userId": "06069808385232535167"
     },
     "user_tz": -780
    },
    "id": "RQ5glR8Qa1VK"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# first identify records by finding two line breaks in sequence\n",
    "unprocessed_corpus = text.split('\\n\\n')\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "corpus_cleaned = []\n",
    "\n",
    "docs = list(nlp.pipe(unprocessed_corpus, batch_size=100))\n",
    "\n",
    "# 'year' appears very frequently, and is not very useful for representing documents\n",
    "exclude = ['year']\n",
    "\n",
    "# this step may take a while\n",
    "for doc in docs:\n",
    "    pos_to_keep = [word for word in doc if word.tag_ not in ['NNP', 'NNPS', 'PRP$', 'CD', 'DT', 'CC', 'IN', 'RP', 'TO', 'UH', 'PUNCT']]\n",
    "    tokens_to_keep = [word.text.lower() for word in pos_to_keep if word.is_alpha and word.text not in exclude]\n",
    "    corpus_cleaned.append(tokens_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We'll make two versions. In one, each document is split into a list of token:\n",
    "eg ['the', 'island', 'of', 'britain', 'is', '800', 'miles', 'long'...]\n",
    "\n",
    "In the other, each document is joined together as a single string of text\n",
    "eg 'the island of britain is 800 miles long...'\n",
    "'''\n",
    "\n",
    "corpus_split = corpus_cleaned\n",
    "corpus_joined = [(' ').join(each) for each in corpus_cleaned]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-zVUhU2g5qm"
   },
   "source": [
    "Now we can see what effect the pre-processing has had. Below we'll print one document in the corpus, and also take another look at the overall term frequency list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 303,
     "status": "ok",
     "timestamp": 1707602312699,
     "user": {
      "displayName": "Christopher Thomson",
      "userId": "06069808385232535167"
     },
     "user_tz": -780
    },
    "id": "vBSL1EalaWdq",
    "outputId": "7521a7c1-863c-4849-be2c-002afebce55d"
   },
   "outputs": [],
   "source": [
    "# inspect the first document\n",
    "print(corpus_split[0])\n",
    "print(corpus_joined[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 324,
     "status": "ok",
     "timestamp": 1707602347432,
     "user": {
      "displayName": "Christopher Thomson",
      "userId": "06069808385232535167"
     },
     "user_tz": -780
    },
    "id": "nRoBl59Kau_v",
    "outputId": "f922a1d9-7299-436f-b902-b0b7550f729b"
   },
   "outputs": [],
   "source": [
    "cleaned_corpus_words = []\n",
    "for word_list in corpus_split:\n",
    "  cleaned_corpus_words.extend(word_list)\n",
    "\n",
    "counts = collections.Counter(cleaned_corpus_words)\n",
    "counts.most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4I7UVJnGIPIi"
   },
   "source": [
    "Let's understand how long our texts are...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 309,
     "status": "ok",
     "timestamp": 1707602414636,
     "user": {
      "displayName": "Christopher Thomson",
      "userId": "06069808385232535167"
     },
     "user_tz": -780
    },
    "id": "nWCQzfmVtSE_",
    "outputId": "156154a0-8474-44a2-c55b-bc8e13469bff"
   },
   "outputs": [],
   "source": [
    "print(\"There are\", len(corpus_joined), \"documents in the corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YLhv4zWzIfwL"
   },
   "source": [
    "What is their average length?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "executionInfo": {
     "elapsed": 840,
     "status": "ok",
     "timestamp": 1707602419393,
     "user": {
      "displayName": "Christopher Thomson",
      "userId": "06069808385232535167"
     },
     "user_tz": -780
    },
    "id": "e5acIsrgEl3v",
    "outputId": "9e9033cc-3848-45eb-949e-9da9e350a9d6"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "lengths = []\n",
    "\n",
    "for record in unprocessed_corpus:\n",
    "    lengths.append(len(record.split(' ')))\n",
    "\n",
    "lengths_df = pd.DataFrame(lengths)\n",
    "lengths_df.plot(kind='box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 322,
     "status": "ok",
     "timestamp": 1707602542334,
     "user": {
      "displayName": "Christopher Thomson",
      "userId": "06069808385232535167"
     },
     "user_tz": -780
    },
    "id": "a-U5g4pDrlv7",
    "outputId": "30fced66-1019-471c-bb66-2c684a261d77"
   },
   "outputs": [],
   "source": [
    "print(\"Shortest record is:\", min(lengths), \"words.\")\n",
    "print(\"Longest record is:\", max(lengths), \"words.\")\n",
    "print(\"The mean length is:\", np.mean(lengths), \"words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JnMyXy_qRibr"
   },
   "source": [
    "In general, most records are short, around 150 words, but there are some outliers ranging up as high as nearly 1900 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QaAW9tIs1SKF"
   },
   "source": [
    "### Term Frequency - Inverse Document Frequency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 291,
     "status": "ok",
     "timestamp": 1707603554882,
     "user": {
      "displayName": "Christopher Thomson",
      "userId": "06069808385232535167"
     },
     "user_tz": -780
    },
    "id": "gEe7zwvnlaZj"
   },
   "outputs": [],
   "source": [
    "# This code cell is adapted from Steven Loria's TF-IDF example using the Textblob package\n",
    "# Just for illustration purposes - we will later use Sci-kit Learn to calculate this for us\n",
    "\n",
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "\n",
    "def raw_tf(word, doc):\n",
    "    return doc.words.count(word)\n",
    "\n",
    "def rel_tf(word, doc):\n",
    "    return doc.words.count(word) / len(doc.words)\n",
    "\n",
    "def doc_frequency(word, corpus):\n",
    "    return sum(1 for doc in corpus if word in doc)\n",
    "\n",
    "def idf(word, corpus):\n",
    "    # this takes the natural logarithm\n",
    "    return math.log(len(corpus) / (1 + doc_frequency(word, corpus)))\n",
    "\n",
    "def tfidf(word, doc, corpus):\n",
    "    return rel_tf(word, doc) * idf(word, corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uk_ADOqbmDA8"
   },
   "source": [
    "Let's look at a single document (`example_1` in our code below is the first annal). We'll print out the term and document frequencies in order to see all the values contributing to our TF-IDF score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_joined[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 677
    },
    "executionInfo": {
     "elapsed": 327,
     "status": "ok",
     "timestamp": 1707603814250,
     "user": {
      "displayName": "Christopher Thomson",
      "userId": "06069808385232535167"
     },
     "user_tz": -780
    },
    "id": "ncXNA33RfE7f",
    "outputId": "845a2582-d74a-4177-cf90-4fceba1b75ff"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "example_1 = tb(corpus_joined[1])\n",
    "\n",
    "results = []\n",
    "\n",
    "for word in example_1.words:\n",
    "  raw_freq = raw_tf(word, example_1)\n",
    "  rel_freq = rel_tf(word, example_1)\n",
    "  doc_freq = doc_frequency(word, corpus_joined)\n",
    "  inverse_doc_freq = idf(word, corpus_joined)\n",
    "  tf_idf = tfidf(word, example_1, corpus_joined)\n",
    "\n",
    "  example_1_scores = {'word': '{}'.format(word),\n",
    "            'raw_frequency': '{}'.format(raw_freq),\n",
    "            'rel_frequency': '{}'.format(rel_freq),\n",
    "            'document_frequency': '{}'.format(doc_freq),\n",
    "            'IDF': '{}'.format(inverse_doc_freq),\n",
    "            'TF-IDF': '{}'.format(tf_idf)}\n",
    "  results.append(example_1_scores)\n",
    "\n",
    "example_1_df = pd.DataFrame(results)\n",
    "# here we have to ensure the TF-IDF scores are treated as numbers, not text\n",
    "example_1_df['TF-IDF'] = pd.to_numeric(example_1_df[\"TF-IDF\"])\n",
    "\n",
    "unique = example_1_df[['word', 'raw_frequency', 'rel_frequency', 'document_frequency', 'IDF', 'TF-IDF']].drop_duplicates()\n",
    "\n",
    "#unique.head(20)\n",
    "unique.sort_values(by='TF-IDF', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points to note:\n",
    "\n",
    "- Raw frequency is the count for this token in this document.\n",
    "- Document frequency is the number of documents where this token appears in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fog82O59f3lB"
   },
   "source": [
    "Let's just check the maths with one example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_tf('ford', example_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc frequency\n",
    "sum(1 for doc in corpus_joined if 'ford' in doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math.log(len(corpus_joined) / (1 + doc_frequency('ford', corpus_joined)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_tf('ford', example_1) * idf('ford', corpus_joined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MT4AVc6vpytF"
   },
   "source": [
    "We can also make lists of documents and their most distinctive words, as measured using TF-IDF. This gives us a sense of the usefulness of the TF-IDF score, and how these can provide discriminative features for documents. For example, the word \"island\" has a high score in documents 1 and 6 below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 528,
     "status": "ok",
     "timestamp": 1707603892825,
     "user": {
      "displayName": "Christopher Thomson",
      "userId": "06069808385232535167"
     },
     "user_tz": -780
    },
    "id": "43rfQav1eIGh",
    "outputId": "dd413152-8954-41a3-fd50-b077831c615d"
   },
   "outputs": [],
   "source": [
    "for i, doc in enumerate(corpus_joined[:21:4]):\n",
    "    blob = tb(doc)\n",
    "    print(\"--------\")\n",
    "    print(\"Top words in document {}\".format(i + 1))\n",
    "    scores = {word: tfidf(word, blob, corpus_joined) for word in blob.words}\n",
    "    sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    for word, score in sorted_words[:10]:\n",
    "        print(\"Word: {}, TF-IDF: {}\".format(word, round(score, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FLTx35KitHsY"
   },
   "source": [
    "### TF-IDF scores for our whole corpus\n",
    "\n",
    "Let's go back a few steps and introduce some a package that will calculate TF-IDF for our whole corpus more efficiently. We will use Sci-Kit Learn, a very popular machine learning package.\n",
    "\n",
    "- This will produce TF-IDF scores for each document where each column is a document, and each row is a word.\n",
    "\n",
    "- We will remove grammatical words (`stop_words='english'`) because these won't help us measure similarity here.\n",
    "\n",
    "- We can apply a minimum document frequency threshold. This further reduces the size of our data by excluding words that appear in only a few documents. However, take note of which words will be excluded by doing this (eg 'female' in the first record). Computational method motivations can be at odds with our aims sometimes!\n",
    "\n",
    "- In addition, Sci-kit Learn will normalise these scores, so that the sum of squares for each document's scores (i.e., each row)  is equal to 1.\n",
    "\n",
    "- You'll also see lots of zeros in the data we produce. This occurs when a word does not appear in a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 412
    },
    "executionInfo": {
     "elapsed": 316,
     "status": "ok",
     "timestamp": 1707603983765,
     "user": {
      "displayName": "Christopher Thomson",
      "userId": "06069808385232535167"
     },
     "user_tz": -780
    },
    "id": "6MvG0dCnccge",
    "outputId": "354e26cc-03fb-42fc-c2ae-cb6321d09cc8"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english',min_df=5)\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus_joined)\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(),\n",
    "                      columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Show us the first 10 rows\n",
    "tfidf_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the min_df above and uncomment the next line to find the sole mention of 'female' in the corpus\n",
    "# tfidf_df[tfidf_df['female']>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0jHDsArtWzfj"
   },
   "source": [
    "You might like to find which rows contain a score for a given word. For example, try filtering the data in the cell below.\n",
    "\n",
    "- king, bishop\n",
    "- summer, winter\n",
    "- son, daughter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XeLMh9aK1l4O"
   },
   "outputs": [],
   "source": [
    "q = 'king'\n",
    "idx = tfidf_df.columns.tolist().index(q)\n",
    "col = tfidf_df.iloc[:, idx]\n",
    "\n",
    "result = tfidf_df[col.gt(0)].iloc[:, idx]\n",
    "\n",
    "print(\"Number of results for\", q, \"is\", len(result))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u0vMIhOnYOkV"
   },
   "source": [
    "### Measuring Similarity\n",
    "\n",
    "How can we measure similarity from the document-term matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 315,
     "status": "ok",
     "timestamp": 1707604647939,
     "user": {
      "displayName": "Christopher Thomson",
      "userId": "06069808385232535167"
     },
     "user_tz": -780
    },
    "id": "5BcR0ML1ckoz"
   },
   "outputs": [],
   "source": [
    "# a code snippet to help make output later in this section readable!\n",
    "import textwrap\n",
    "\n",
    "wrapper = textwrap.TextWrapper(width=35,\n",
    "    initial_indent=\" \" * 4,\n",
    "    subsequent_indent=\" \" * 4,\n",
    "    break_long_words=False,\n",
    "    break_on_hyphens=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_GCbcIbwBmd"
   },
   "source": [
    "The first step is to compute a similarity matrix. For this we use cosine similarity, which measures the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 526,
     "status": "ok",
     "timestamp": 1707604751219,
     "user": {
      "displayName": "Christopher Thomson",
      "userId": "06069808385232535167"
     },
     "user_tz": -780
    },
    "id": "ckA0H0Ygm1jc",
    "outputId": "92c35f64-5a1a-4151-e100-2e88d01d9551"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# compute a similarity matrix\n",
    "# notice its symmetry along the diagonal\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "print(cosine_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bdw0NQnWpj0W"
   },
   "source": [
    "This similarity matrix compares the TF-IDF vectors for each document with every other document. This  the cosine similarity in each case. The outcome is a similarity matrix that summarises the similarity between documents for the whole corpus.\n",
    "\n",
    "The similarity matrix can be used to find which documents are most similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 295,
     "status": "ok",
     "timestamp": 1707605173379,
     "user": {
      "displayName": "Christopher Thomson",
      "userId": "06069808385232535167"
     },
     "user_tz": -780
    },
    "id": "9klf8yqhYEac",
    "outputId": "069797ff-7d43-47b3-fad6-1c8fa3da01b8"
   },
   "outputs": [],
   "source": [
    "# specify a target document and find similar documents to it (eg try 20, 85, 483)\n",
    "target_doc = 105\n",
    "\n",
    "sim_scores = list(enumerate(cosine_sim[target_doc]))\n",
    "sorted_sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "sorted_sim_scores[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 316,
     "status": "ok",
     "timestamp": 1707605187321,
     "user": {
      "displayName": "Christopher Thomson",
      "userId": "06069808385232535167"
     },
     "user_tz": -780
    },
    "id": "Brh_65CBZ6yo",
    "outputId": "505b8a58-e941-453c-db3c-b0a816daf389"
   },
   "outputs": [],
   "source": [
    "print('Documents similar to doc {}:'.format(target_doc))\n",
    "print('The first result is the target document itself!')\n",
    "print('\\n')\n",
    "\n",
    "for doc in sorted_sim_scores[:6]:\n",
    "  print(wrapper.fill(unprocessed_corpus[doc[0]][:500]))\n",
    "  print('\\n')\n",
    "  print('-----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UiGt3tb5wO-j"
   },
   "source": [
    "### Task\n",
    "\n",
    "Make some notes here about an example you looked at. \n",
    "\n",
    "- Which document features (words) are driving the similarity?\n",
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aBvAwlkbKzGu"
   },
   "source": [
    "### Clustering and visualisation\n",
    "\n",
    "We'll make some clusters based on Tf-idf similarity, and see if it produces useful results. The size of clusters is quite arbitrary, but the idea is that clusters should form around documents with similar representations in the document-term matrix.\n",
    "\n",
    "First, we'll use Principal Components Analysis to project the variation in our data into two dimensions. This allows us to see some of main differences between documents (though with some information loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1190,
     "status": "ok",
     "timestamp": 1707605378209,
     "user": {
      "displayName": "Christopher Thomson",
      "userId": "06069808385232535167"
     },
     "user_tz": -780
    },
    "id": "ZYKS-qnAREl1",
    "outputId": "9efa8334-60da-47dc-aeb6-15d5e2cfedc5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Set PCA to 2D [sklearn]\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Name of Vector Array (Numpy)\n",
    "name_of_vector_array = tfidf_matrix.toarray()\n",
    "\n",
    "\n",
    "# New D2 Dataframe (PCA)\n",
    "# PCA components are a composite of variation across many dimensions\n",
    "df2d = pd.DataFrame(pca.fit_transform(name_of_vector_array), columns=['component1', 'component2'])\n",
    "\n",
    "# Plot Data Visualization (Matplotlib)\n",
    "df2d.plot(kind='scatter', x='component1', y='component2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import seaborn as sns\n",
    "\n",
    "kmeans = KMeans(n_clusters=10, random_state=1, n_init=\"auto\").fit(df2d)\n",
    "kmeans.labels_\n",
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2d['cluster'] = pd.Categorical(kmeans.labels_)\n",
    "sns.scatterplot(x=\"component1\",y=\"component2\",hue=\"cluster\",data=df2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z1q4eHCl3Y08"
   },
   "source": [
    "Below we investigate documents in the first cluster (cluster 0), along with the features by which they are represented.\n",
    "\n",
    "Change the value of `cluster_number` below to investigate other cluster groupings. The clusters tend to include records from years that are close together. These often include similar content, and may have been written by the same scribe or in a 'contemporary' style by a number of scribes. You may also notice similarities through certain words, eg the presence of 'Britain' or 'succeeded' etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "- Try finding some distinctive clusters. What do they tell us about what constituted an important historical event in Anglo-Saxon England?\n",
    "- Try creating some different number of clusters by changing ```n_clusters``` above and explore the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1707605679266,
     "user": {
      "displayName": "Christopher Thomson",
      "userId": "06069808385232535167"
     },
     "user_tz": -780
    },
    "id": "idbVdTJgK4im",
    "outputId": "4ef3ca4d-a4e5-4199-d421-7d0b682eb396"
   },
   "outputs": [],
   "source": [
    "cluster_number = 8\n",
    "cluster_index = df2d[df2d['cluster'] == cluster_number].index.tolist()\n",
    "\n",
    "# print the first 500 tokens of the first 10 records per cluster\n",
    "for i in cluster_index[:10]:\n",
    "    print(\"Text:\", unprocessed_corpus[i][:500])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iNhvI8K-4XEC"
   },
   "source": [
    "We can display the word features important in the selected cluster, and their frequency overall in that cluster, using a word cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "executionInfo": {
     "elapsed": 1864,
     "status": "ok",
     "timestamp": 1707605708673,
     "user": {
      "displayName": "Christopher Thomson",
      "userId": "06069808385232535167"
     },
     "user_tz": -780
    },
    "id": "SjN_bT7b6QLm",
    "outputId": "79ebbb4f-7c90-4a62-fc5b-f52d69b2af6f"
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "combined_cluster_features = []\n",
    "for idx in cluster_index:\n",
    "    combined_cluster_features.extend(corpus_split[idx])\n",
    "    \n",
    "t = (' ').join(combined_cluster_features)\n",
    "\n",
    "wc = WordCloud(background_color=\"white\",\n",
    "               color_func=lambda *args, **kwargs: \"black\").generate_from_text(t)\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b4pRxVNWh6-v"
   },
   "source": [
    "### References\n",
    "\n",
    "[1]. *The Anglo-Saxon Chronicle*. Translation by Rev. James Ingram (London, 1823), with additional\n",
    "readings from the translation of Dr. J.A. Giles (London, 1847).(https://www.gutenberg.org/cache/epub/657/pg657.txt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rlakpI-ztmUz"
   },
   "source": [
    "[2]. Barbara McGillivray, Gábor Mihály Tóth. *Applying Language Technology in Humanities Research*. Palgrave Macmillan, 2020. https://doi.org/10.1007/978-3-030-46493-6"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP9opD/2SYVObEG09O23abI",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
